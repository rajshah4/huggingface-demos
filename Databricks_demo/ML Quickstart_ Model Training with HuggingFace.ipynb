{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"528c6d2b-3c5a-4236-ac61-fa8cc8d4d323","showTitle":false,"title":""}},"source":["# Databricks & Hugging Face ML Quickstart: Model Training\n","\n","This notebook provides a quick overview of machine learning model training on Databricks using Hugging Face transformers. The notebook includes using MLflow to track the trained models.\n","\n","This tutorial covers:\n","- Part 1: Training a text classification transformer model with MLflow tracking\n","\n","### Requirements\n","- Cluster running Databricks Runtime 7.5 ML or above\n","- Training is super slow/unusable if there is no GPU attached to the cluster"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8738a402-24dc-4074-bebb-b51bec8e74db","showTitle":false,"title":""}},"source":["### Libraries\n","Import the necessary libraries. These libraries are preinstalled on Databricks Runtime for Machine Learning ([AWS](https://docs.databricks.com/runtime/mlruntime.html)|[Azure](https://docs.microsoft.com/azure/databricks/runtime/mlruntime)|[GCP](https://docs.gcp.databricks.com/runtime/mlruntime.html)) clusters and are tuned for compatibility and performance."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7c554af6-20e3-44ec-a8e1-b1e3411ab169","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Python interpreter will be restarted.\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","Collecting datasets\n","  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n","Collecting mlflow\n","  Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB)\n","Collecting torch\n","  Downloading torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","Collecting dill<0.3.6\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.8/site-packages (from datasets) (1.20.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","Collecting pyarrow>=6.0.0\n","  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n","Requirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from datasets) (20.9)\n","Requirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from datasets) (1.2.4)\n","Collecting tqdm>=4.62.1\n","  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","Requirement already satisfied: requests>=2.19.0 in /databricks/python3/lib/python3.8/site-packages (from datasets) (2.25.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Collecting typing-extensions>=3.7.4.3\n","  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\n","Collecting Flask\n","  Downloading Flask-2.1.3-py3-none-any.whl (95 kB)\n","Requirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\n","Collecting alembic\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","Collecting sqlparse>=0.3.1\n","  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\n","Requirement already satisfied: protobuf>=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\n","Collecting gitpython>=2.1.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","Collecting databricks-cli>=0.8.7\n","  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\n","Collecting cloudpickle\n","  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n","Collecting click>=7.0\n","  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n","Collecting sqlalchemy>=1.4.0\n","  Downloading SQLAlchemy-1.4.39-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","Collecting gunicorn\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","Collecting docker>=4.0.0\n","  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n","Collecting querystring-parser\n","  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n","Collecting importlib-metadata!=4.7.0,>=3.7.0\n","  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n","Requirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\n","Collecting prometheus-flask-exporter\n","  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n","Collecting pyjwt>=1.7.0\n","  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n","Collecting oauthlib>=3.1.0\n","  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n","Collecting tabulate>=0.7.7\n","  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n","Requirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n","Collecting websocket-client>=0.32.0\n","  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n","Collecting greenlet!=0.4.17\n","  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n","Collecting regex!=2019.12.17\n","  Downloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting charset-normalizer<3.0,>=2.0\n","  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting importlib-resources\n","  Downloading importlib_resources-5.8.0-py3-none-any.whl (28 kB)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","Collecting Werkzeug>=2.0\n","  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n","Collecting itsdangerous>=2.0\n","  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n","Collecting Jinja2>=3.0\n","  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2>=3.0->Flask->mlflow) (2.0.1)\n","Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn->mlflow) (52.0.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: prometheus-client in /databricks/python3/lib/python3.8/site-packages (from prometheus-flask-exporter->mlflow) (0.10.1)\n","Building wheels for collected packages: databricks-cli\n","  Building wheel for databricks-cli (setup.py): started\n","  Building wheel for databricks-cli (setup.py): finished with status 'done'\n","  Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141932 sha256=76e05fda839c92dd5a17919cf47935b01a503d804ab7171d9d428ba7da21dc32\n","  Stored in directory: /root/.cache/pip/wheels/bc/ef/2a/18885b70c6b78d4b9612ef2bf4bfdc7325f43db9d817d20f3f\n","Successfully built databricks-cli\n","Installing collected packages: zipp, multidict, frozenlist, yarl, Werkzeug, smmap, Jinja2, itsdangerous, importlib-metadata, greenlet, click, charset-normalizer, async-timeout, aiosignal, websocket-client, typing-extensions, tqdm, tabulate, sqlalchemy, pyyaml, pyjwt, oauthlib, Mako, importlib-resources, gitdb, fsspec, Flask, dill, aiohttp, xxhash, tokenizers, sqlparse, responses, regex, querystring-parser, pyarrow, prometheus-flask-exporter, multiprocess, huggingface-hub, gunicorn, gitpython, docker, databricks-cli, cloudpickle, alembic, transformers, torch, mlflow, datasets\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Not uninstalling jinja2 at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9\n","    Can't uninstall 'Jinja2'. No files were found to uninstall.\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 4.0.0\n","    Not uninstalling pyarrow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9\n","    Can't uninstall 'pyarrow'. No files were found to uninstall.\n","Successfully installed Flask-2.1.3 Jinja2-3.1.2 Mako-1.2.1 Werkzeug-2.1.2 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.8.1 async-timeout-4.0.2 charset-normalizer-2.1.0 click-8.1.3 cloudpickle-2.1.0 databricks-cli-0.17.0 datasets-2.3.2 dill-0.3.5.1 docker-5.0.3 frozenlist-1.3.0 fsspec-2022.5.0 gitdb-4.0.9 gitpython-3.1.27 greenlet-1.1.2 gunicorn-20.1.0 huggingface-hub-0.8.1 importlib-metadata-4.12.0 importlib-resources-5.8.0 itsdangerous-2.1.2 mlflow-1.27.0 multidict-6.0.2 multiprocess-0.70.13 oauthlib-3.2.0 prometheus-flask-exporter-0.20.2 pyarrow-8.0.0 pyjwt-2.4.0 pyyaml-6.0 querystring-parser-1.2.4 regex-2022.7.9 responses-0.18.0 smmap-5.0.0 sqlalchemy-1.4.39 sqlparse-0.4.2 tabulate-0.8.10 tokenizers-0.12.1 torch-1.12.0 tqdm-4.64.0 transformers-4.20.1 typing-extensions-4.3.0 websocket-client-1.3.3 xxhash-3.0.0 yarl-1.7.2 zipp-3.8.1\n","Python interpreter will be restarted.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Python interpreter will be restarted.\nCollecting transformers\n  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\nCollecting datasets\n  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\nCollecting mlflow\n  Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB)\nCollecting torch\n  Downloading torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\nCollecting xxhash\n  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\nCollecting aiohttp\n  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\nCollecting fsspec[http]>=2021.05.0\n  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\nCollecting dill<0.3.6\n  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.8/site-packages (from datasets) (1.20.1)\nCollecting multiprocess\n  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\nCollecting responses<0.19\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nCollecting huggingface-hub<1.0.0,>=0.1.0\n  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\nCollecting pyarrow>=6.0.0\n  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from datasets) (20.9)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from datasets) (1.2.4)\nCollecting tqdm>=4.62.1\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: requests>=2.19.0 in /databricks/python3/lib/python3.8/site-packages (from datasets) (2.25.1)\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nCollecting Flask\n  Downloading Flask-2.1.3-py3-none-any.whl (95 kB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nCollecting alembic\n  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\nCollecting sqlparse>=0.3.1\n  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\nRequirement already satisfied: protobuf>=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nCollecting gitpython>=2.1.0\n  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\nCollecting databricks-cli>=0.8.7\n  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\nCollecting cloudpickle\n  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\nCollecting click>=7.0\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\nCollecting sqlalchemy>=1.4.0\n  Downloading SQLAlchemy-1.4.39-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting gunicorn\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker>=4.0.0\n  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting importlib-metadata!=4.7.0,>=3.7.0\n  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\nCollecting pyjwt>=1.7.0\n  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\nCollecting oauthlib>=3.1.0\n  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\nCollecting tabulate>=0.7.7\n  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\nCollecting websocket-client>=0.32.0\n  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nCollecting zipp>=0.5\n  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\nCollecting regex!=2019.12.17\n  Downloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\nCollecting async-timeout<5.0,>=4.0.0a3\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting charset-normalizer<3.0,>=2.0\n  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\nCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\nCollecting importlib-resources\n  Downloading importlib_resources-5.8.0-py3-none-any.whl (28 kB)\nCollecting Mako\n  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\nCollecting Werkzeug>=2.0\n  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\nCollecting itsdangerous>=2.0\n  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting Jinja2>=3.0\n  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2>=3.0->Flask->mlflow) (2.0.1)\nRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn->mlflow) (52.0.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: prometheus-client in /databricks/python3/lib/python3.8/site-packages (from prometheus-flask-exporter->mlflow) (0.10.1)\nBuilding wheels for collected packages: databricks-cli\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status 'done'\n  Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141932 sha256=76e05fda839c92dd5a17919cf47935b01a503d804ab7171d9d428ba7da21dc32\n  Stored in directory: /root/.cache/pip/wheels/bc/ef/2a/18885b70c6b78d4b9612ef2bf4bfdc7325f43db9d817d20f3f\nSuccessfully built databricks-cli\nInstalling collected packages: zipp, multidict, frozenlist, yarl, Werkzeug, smmap, Jinja2, itsdangerous, importlib-metadata, greenlet, click, charset-normalizer, async-timeout, aiosignal, websocket-client, typing-extensions, tqdm, tabulate, sqlalchemy, pyyaml, pyjwt, oauthlib, Mako, importlib-resources, gitdb, fsspec, Flask, dill, aiohttp, xxhash, tokenizers, sqlparse, responses, regex, querystring-parser, pyarrow, prometheus-flask-exporter, multiprocess, huggingface-hub, gunicorn, gitpython, docker, databricks-cli, cloudpickle, alembic, transformers, torch, mlflow, datasets\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 4.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\nSuccessfully installed Flask-2.1.3 Jinja2-3.1.2 Mako-1.2.1 Werkzeug-2.1.2 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.8.1 async-timeout-4.0.2 charset-normalizer-2.1.0 click-8.1.3 cloudpickle-2.1.0 databricks-cli-0.17.0 datasets-2.3.2 dill-0.3.5.1 docker-5.0.3 frozenlist-1.3.0 fsspec-2022.5.0 gitdb-4.0.9 gitpython-3.1.27 greenlet-1.1.2 gunicorn-20.1.0 huggingface-hub-0.8.1 importlib-metadata-4.12.0 importlib-resources-5.8.0 itsdangerous-2.1.2 mlflow-1.27.0 multidict-6.0.2 multiprocess-0.70.13 oauthlib-3.2.0 prometheus-flask-exporter-0.20.2 pyarrow-8.0.0 pyjwt-2.4.0 pyyaml-6.0 querystring-parser-1.2.4 regex-2022.7.9 responses-0.18.0 smmap-5.0.0 sqlalchemy-1.4.39 sqlparse-0.4.2 tabulate-0.8.10 tokenizers-0.12.1 torch-1.12.0 tqdm-4.64.0 transformers-4.20.1 typing-extensions-4.3.0 websocket-client-1.3.3 xxhash-3.0.0 yarl-1.7.2 zipp-3.8.1\nPython interpreter will be restarted.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%pip install transformers datasets mlflow torch"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8a18992e-ce3f-4b09-a6a1-ddd867006afa","showTitle":false,"title":""}},"source":["### Install Git LFS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"27caf826-3804-40f1-8cd8-bc72077ceeb0","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Detected operating system as Ubuntu/focal.\n","Checking for curl...\n","Detected curl...\n","Checking for gpg...\n","Detected gpg...\n","Running apt-get update... done.\n","Installing apt-transport-https... done.\n","Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n","Importing packagecloud gpg key... done.\n","Running apt-get update... done.\n","\n","The repository is setup! You can now install packages.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","The following NEW packages will be installed:\n","  git-lfs\n","0 upgraded, 1 newly installed, 0 to remove and 68 not upgraded.\n","Need to get 7,168 kB of archives.\n","After this operation, 15.6 MB of additional disk space will be used.\n","Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.2.0 [7,168 kB]\n","debconf: delaying package configuration, since apt-utils is not installed\n","Fetched 7,168 kB in 0s (16.6 MB/s)\n","Selecting previously unselected package git-lfs.\n","(Reading database ... \n","(Reading database ... 5%\n","(Reading database ... 10%\n","(Reading database ... 15%\n","(Reading database ... 20%\n","(Reading database ... 25%\n","(Reading database ... 30%\n","(Reading database ... 35%\n","(Reading database ... 40%\n","(Reading database ... 45%\n","(Reading database ... 50%\n","(Reading database ... 55%\n","(Reading database ... 60%\n","(Reading database ... 65%\n","(Reading database ... 70%\n","(Reading database ... 75%\n","(Reading database ... 80%\n","(Reading database ... 85%\n","(Reading database ... 90%\n","(Reading database ... 95%\n","(Reading database ... 100%\n","(Reading database ... 92257 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_3.2.0_amd64.deb ...\n","Unpacking git-lfs (3.2.0) ...\n","Setting up git-lfs (3.2.0) ...\n","Git LFS initialized.\n","Processing triggers for man-db (2.9.1-1) ...\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Detected operating system as Ubuntu/focal.\nChecking for curl...\nDetected curl...\nChecking for gpg...\nDetected gpg...\nRunning apt-get update... done.\nInstalling apt-transport-https... done.\nInstalling /etc/apt/sources.list.d/github_git-lfs.list...done.\nImporting packagecloud gpg key... done.\nRunning apt-get update... done.\n\nThe repository is setup! You can now install packages.\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following NEW packages will be installed:\n  git-lfs\n0 upgraded, 1 newly installed, 0 to remove and 68 not upgraded.\nNeed to get 7,168 kB of archives.\nAfter this operation, 15.6 MB of additional disk space will be used.\nGet:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.2.0 [7,168 kB]\ndebconf: delaying package configuration, since apt-utils is not installed\nFetched 7,168 kB in 0s (16.6 MB/s)\nSelecting previously unselected package git-lfs.\n(Reading database ... \n(Reading database ... 5%\n(Reading database ... 10%\n(Reading database ... 15%\n(Reading database ... 20%\n(Reading database ... 25%\n(Reading database ... 30%\n(Reading database ... 35%\n(Reading database ... 40%\n(Reading database ... 45%\n(Reading database ... 50%\n(Reading database ... 55%\n(Reading database ... 60%\n(Reading database ... 65%\n(Reading database ... 70%\n(Reading database ... 75%\n(Reading database ... 80%\n(Reading database ... 85%\n(Reading database ... 90%\n(Reading database ... 95%\n(Reading database ... 100%\n(Reading database ... 92257 files and directories currently installed.)\nPreparing to unpack .../git-lfs_3.2.0_amd64.deb ...\nUnpacking git-lfs (3.2.0) ...\nSetting up git-lfs (3.2.0) ...\nGit LFS initialized.\nProcessing triggers for man-db (2.9.1-1) ...\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%sh\n","curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","sudo apt-get install git-lfs"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cec61d6d-ea1a-4d2f-9ee6-625393a24aa5","showTitle":false,"title":""}},"outputs":[],"source":["import mlflow\n","import torch\n","#from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n","#from hyperopt.pyll import scope\n","from datasets import load_dataset, load_metric\n","from huggingface_hub import notebook_login\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n",")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"094921b5-f746-4303-af4b-7a4d61b3b48a","showTitle":false,"title":""}},"source":["### Log into Hugging Face Hub"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4af8d844-c5f0-45c0-80c8-c0b74e55abe9","showTitle":false,"title":""}},"source":["This uses the command line to login into the hugging face hub.  If the Hugging Face hub is private, specify the location using the \"HF_ENDPOINT\" parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"896afdc7-85b9-4ad0-ac9e-2a68def84532","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Login successful\nYour token has been saved to /root/.huggingface/token\n\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\u001b[0m\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["from huggingface_hub.commands.user import _login\n","from huggingface_hub import HfApi\n","api = HfApi()\n","_login(hf_api = api, token = \"API TOKEN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0a3fc9dd-7a03-41c4-a43a-6a2a3ee610bd","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["rajistics\r\n","\u001b[1morgs: \u001b[0m huggingface,spaces-explorers,demo-org,HF-test-lab,qualitydatalab\r\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"rajistics\r\n\u001b[1morgs: \u001b[0m huggingface,spaces-explorers,demo-org,HF-test-lab,qualitydatalab\r\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["#Verify Login\n","!huggingface-cli whoami"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b2f67ffe-ad1b-49a1-a7cf-603daa8c9890","showTitle":false,"title":""}},"source":["### Load data\n","The tutorial uses the IMDB dataset for move reviews.  The complete [model card](https://huggingface.co/datasets/imdb) can be found at Hugging Face with details on the dataset. \n","\n","The goal is to classify reviews as positive or negative. \n","\n","The dataset is loaded using the Hugging Face datasets package."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"51fd8a9f-bef3-4fbd-90ce-8531f5f71205","showTitle":false,"title":""}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d4ca296ee794c1a8d94ce2b05d8aef8","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d4ca296ee794c1a8d94ce2b05d8aef8","version_major":2,"version_minor":0},"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7e9d738652046ffb8fe6e56e9e58b55","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7e9d738652046ffb8fe6e56e9e58b55","version_major":2,"version_minor":0},"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"text/plain":["Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b843bb85b94a4bf3bb1dcf314b50a0eb","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b843bb85b94a4bf3bb1dcf314b50a0eb","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a97f14b9f59247768cdf8262606f0989","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a97f14b9f59247768cdf8262606f0989","version_major":2,"version_minor":0},"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21c15d1e4d0a4cbaab0786ad78f6acf3","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21c15d1e4d0a4cbaab0786ad78f6acf3","version_major":2,"version_minor":0},"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a080ed5a85741c3ae1f2f1099c2dbe6","version_major":2,"version_minor":0},"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a080ed5a85741c3ae1f2f1099c2dbe6","version_major":2,"version_minor":0},"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"text/plain":["Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9d77918a5514c6488f87a4346fd868e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9d77918a5514c6488f87a4346fd868e","version_major":2,"version_minor":0},"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"}],"source":["# Load and preprocess data\n","train_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8123df40-67fa-43d0-97c1-6f608c9f7d61","showTitle":false,"title":""}},"source":["## Part 1. Train a classification model"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f3441df0-a7fe-4d6d-bf62-46184cf6ac2c","showTitle":false,"title":""}},"source":["### MLflow Tracking\n","[MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html) allows you to organize your machine learning training code, parameters, and models. \n","\n","You can enable automatic MLflow tracking by using [*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"18e0381e-7c02-4e57-b29b-127a7585992b","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["2022/07/20 18:58:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n","2022/07/20 18:58:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n","'JavaPackage' object is not callable\n","2022/07/20 18:58:22 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n","'JavaPackage' object is not callable\n","2022/07/20 18:58:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n","'JavaPackage' object is not callable\n","2022/07/20 18:58:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"2022/07/20 18:58:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2022/07/20 18:58:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n'JavaPackage' object is not callable\n2022/07/20 18:58:22 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n'JavaPackage' object is not callable\n2022/07/20 18:58:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n'JavaPackage' object is not callable\n2022/07/20 18:58:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# Enable MLflow autologging for this notebook\n","mlflow.autolog()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"88913160-61c4-4cbf-8f2d-235e86fa8768","showTitle":false,"title":""}},"source":["Next, train a classifier within the context of an MLflow run, which automatically logs the trained model and many associated metrics and parameters. You can supplement the logging with additional metrics such as the model's AUC score on the test dataset.\n","If the model is private, another way to access the model is by using the `use_auth_token` parameter to specify the API key that has access to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"70fefb47-9af8-49c8-932d-49a0727c1428","showTitle":false,"title":""}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c058a42effe4e21aaa873254102c1ad","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c058a42effe4e21aaa873254102c1ad","version_major":2,"version_minor":0},"text/plain":"Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d832d9fee224b82a18b21bac9472af1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d832d9fee224b82a18b21bac9472af1","version_major":2,"version_minor":0},"text/plain":"Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"text/plain":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fd5ba8baf2b4cd1b7631926b2571104","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fd5ba8baf2b4cd1b7631926b2571104","version_major":2,"version_minor":0},"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d6fe5dc13745bb9ecc473d80263b8c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d6fe5dc13745bb9ecc473d80263b8c","version_major":2,"version_minor":0},"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bff72684447d426496b902b7f6c89f2a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bff72684447d426496b902b7f6c89f2a","version_major":2,"version_minor":0},"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"}],"source":["model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"85fa6e22-56ab-44da-89c0-e77f883c5fcd","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Parameter 'function'=<function tokenize_function at 0x7fe6690cbd30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Parameter 'function'=<function tokenize_function at 0x7fe6690cbd30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc8cd44f53aa415e89f42be6d56e7097","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc8cd44f53aa415e89f42be6d56e7097","version_major":2,"version_minor":0},"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cdeae94da254a4d91df23d01bf5df7c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cdeae94da254a4d91df23d01bf5df7c","version_major":2,"version_minor":0},"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2a4ebdc91cc441398c435f906d7fa95","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2a4ebdc91cc441398c435f906d7fa95","version_major":2,"version_minor":0},"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"}],"source":["def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","test_dataset = test_dataset.map(tokenize_function, batched=True)\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f1b7f9f9-fd27-497d-81f4-663c43ca5343","showTitle":false,"title":""}},"outputs":[],"source":["training_args = TrainingArguments(\n","    hub_model_id=\"rajistics/distilbert-imdb-mlflow\",\n","    num_train_epochs=1,\n","    output_dir=\"./output\",\n","    logging_steps=500,\n","    save_strategy=\"epoch\",\n","    push_to_hub=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0348ee5b-bd38-440d-9b7f-1fa9150a14b1","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Cloning https://huggingface.co/rajistics/distilbert-imdb-mlflow into local empty directory.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Cloning https://huggingface.co/rajistics/distilbert-imdb-mlflow into local empty directory.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2e107f16-59d1-4cda-ba24-55f271a85755","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n","/local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 25000\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3125\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-1e13fdb6-df80-444a-8fb5-44caac8506d9/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 25000\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3125\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93a6373740994a63bc0a276785797987","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3125 [00:00<?, ?it/s]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93a6373740994a63bc0a276785797987","version_major":2,"version_minor":0},"text/plain":"  0%|          | 0/3125 [00:00<?, ?it/s]"},"datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5bcdb820-3b8f-43f0-8c86-ff34dc5a8bc7","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n","tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n","moviereview = pipeline(\"text-classification\", model = model, tokenizer = tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a7455162-9175-4bfd-bd9c-fcf53b1aa8c1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["moviereview(\"This move was a bit crazy, but I liked it\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"70e02a64-6878-4b9b-9297-5390c9e19ddc","showTitle":false,"title":""}},"source":["### View MLflow runs\n","To view the logged training runs, click the **Experiment** icon at the upper right of the notebook to display the experiment sidebar. If necessary, click the refresh icon to fetch and monitor the latest runs. \n","\n","<img width=\"350\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/experiment-sidebar-icons.png\"/>\n","\n","You can then click the experiment page icon to display the more detailed MLflow experiment page ([AWS](https://docs.databricks.com/applications/mlflow/tracking.html#notebook-experiments)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#notebook-experiments)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/tracking.html#notebook-experiments)). This page allows you to compare runs and view details for specific runs.\n","\n","<img width=\"800\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/compare-runs.png\"/>"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"ML Quickstart: Model Training with HuggingFace","notebookOrigID":3759898664210413,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
